---
layout: post
title: CUDA性能概述：影响因素及优化方法
date: 2025-02-24 +0800
categories: [CUDA]
tags: [CUDA]

# 以下默认false
math: true
mermaid: true
# pin: true
toc:
  sidebar: right

---

性能分析依据`Roofline 模型`，该模型根据`算术强度`（`AI`）划分两种性能区间（以`A100`为例）：

* 内存受限（Memory-bound）：当`AI`低于`13 FLOPs/Byte`时，性能由内存带宽决定。
* 计算受限（Compute-bound）：当`AI`高于`13 FLOPs/Byte`时，性能由计算能力决定。

## 1. 性能影响及优化分类 ##

### 1.1. 内存优化 ###

* `global memory` -- 数据重用：例如将大块内存数据加载到`shared memory`，以减少对`global memory`的访问次数，从而提高`AI`。
* `shared memory` -- `Bank Conflicts`：比如矩阵计算`C = A * B`中的共享内存访问冲突，将`B`矩阵加载到`shared memory`之后，进行转置，避免`Bank Conflicts`。

> 提升`global memory`带宽利用率。由于`warp`访问`global memory`时，是以`128字节`（32x4字节）为单位进行对齐访问，若`warp`内线程访问的地址不连续或未对齐，会导致多次`transaction`，从而降低带宽利用率。建议确保`warp`内线程访问的地址连续且对齐，以实现单次`transaction`访问完整的`128字节`数据。参考：<https://www.olcf.ornl.gov/wp-content/uploads/2020/04/04-CUDA-Fundamental-Optimization-Part-2.pdf>。

> 与`带宽利用率`有关的另一个概念是`全局内存合并访问`（`Global memory coalescing`）。即`warp`内线程访问的地址应连续且对齐，以实现单次`transaction`访问完整的`128字节`数据，从而提升带宽利用率。线程内避免跨步长访问。细节参考：<https://cseweb.ucsd.edu/classes/wi12/cse260-a/Lectures/Lec09.pdf>。

### 1.2. 指令延迟优化 ###

* 线程块并发（`Occupancy`）与延迟隐藏：合理配置共享内存和寄存器使用量，使SM可同时调度多个线程块，提高Warp的调度选择范围，从而隐藏内存访问延迟。
* 线程分叉（`Thread Divergence`）：Warp中线程执行路径不一致会导致序列化执行，降低吞吐率，建议使用分支无关的代码（如min/max替代if-else）以避免分歧。另外，如果数据分块不能完全分配到32个线程，可使用`C+=A*0`替代多余的条件分支。

### 1.3. CPU-GPU 交互优化 ###

* 使用`Stream`实现计算与数据传输重叠（`Overlap`）：使用`Stream`+异步传输+异步启动`kernel`。
* 并发启动多个`kernel`。
* 传输大块内存。
* `pinned memory`：固定分配出一块内存给`CPU`/`GPU`交互使用，禁用内存页管理不会被换出。此时`GPU`驱动可以直接使用`DMA`，传输速度接近理论值。

## 2. 参考资料 ##

* [Introduction to CUDA Performance Optimization](https://developer-blogs.nvidia.com/wp-content/uploads/2024/08/CUDA-Programming-and-Optimization.pdf)：认真看，全面介绍CUDA性能优化，以及以及性能参数计算。
* [NVIDIA -- Fundamental Optimizations in CUDA](https://developer.download.nvidia.cn/GTC/PDF/1083_Wang.pdf)：pdf文档。

## 3. CUDA、CuTe 及其他资料收集 ##

* [CUTLASS: Fast Linear Algebra in CUDA C++](https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/)。强烈推荐，内容对应 GTC2018；详细讲解了在 cutlass 背景下针对 GEMM 优化的分块、内外积转换、缓存，以及相关基础概念和分层设计。
* [GEMM优化博客](https://siboehm.com/articles/22/CUDA-MMM)。强烈推荐，结合性能指标分析从一个最原始的 GEMM 开始优化，注重性能分析
* [nv blog 2025 cutlass](https://developer.nvidia.com/blog/cutlass-principled-abstractions-for-handling-multidimensional-data-through-tensors-and-spatial-microkernels/)。强烈推荐，内容对应 GTC 2023，介绍 cute，结合官方文档的 [CuTe系列](https://docs.nvidia.com/cutlass/media/docs/cpp/cute/index.html) 理解 Layout 设计抽象。
* [nv 2025 blog cutlass 3 介绍](https://developer.nvidia.com/blog/cutlass-3-x-orthogonal-reusable-and-composable-abstractions-for-gemm-kernel-design/)。 3.x 系列引入的特性和抽象分层，对应 GTC 2023，是上文的补充第二部分。
* [GTC 2018：CUTLASS: Software Primitives for Dense Linear Algebra at All Levels and Scales within CUDA](https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8854/)。初次介绍CUTLASS的GTC视频。
* [GTC 2019：PROGRAMMING TENSOR CORES: NATIVE VOLTA TENSOR CORES WITH CUTLASS](https://developer.download.nvidia.cn/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf#page=29.00)。Volta、Turing架构下的cutlass优化。
* [GTC 2021：Accelerating Convolution with Tensor Cores in CUTLASS](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31883/)。卷积支持。
* [GTC 2023：Developing Optimal CUDA Kernels on Hopper Tensor Cores](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51413/)。cutlass 3.x 引入 cute 抽象
* [GTC 2024：CUTLASS: A Performant, Flexible, and Portable Way to Target Hopper Tensor Cores](https://www.nvidia.com/en-us/on-demand/session/gtc24-s61198/)。卷积支持、epilog tree
* [GTC 2025：Programming Blackwell Tensor Cores with CUTLASS](https://www.nvidia.com/en-us/on-demand/session/gtc25-s72720/)。TBC，blackwell 特性。
* [The Present and Future of CUTLASS Tensor Core Programming](https://semianalysis.com/wp-content/uploads/2025/03/The-Present-and-Future-of-CUTLASS-Tensor-Core-Programming-Vijay-Thakkar.pdf)

### 3.1. 其他博客 ###

* [reed CUTE系列](https://www.zhihu.com/column/c_1696937812497235968)
* [杨远航 CUTE 系列笔记](https://www.zhihu.com/column/c_1938664963049763058)
* [写给大家看的 CuTe 教程 -- tiled mma](https://zhuanlan.zhihu.com/p/1937145378446226159)，[系列](https://zhuanlan.zhihu.com/c_1946580149119202264)
* [tri dao 对 flash-attention 3 的介绍](https://tridao.me/blog/2024/flash3/)
