<!DOCTYPE html> <html lang="zh-CN"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CUTLASS-Cute 初步(3.1)：TiledCopy 以及 TiledMMA 配置示例 | Roderick Huang </title> <meta name="author" content="Roderick Huang"> <meta name="description" content="Roderick Huang 的个人博客，记录工作与技术。 "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hxf0223.github.io/blog/2025/Cute%E5%88%9D%E6%AD%A53.1-TileMMA%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Roderick</span> Huang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">CUTLASS-Cute 初步(3.1)：TiledCopy 以及 TiledMMA 配置示例</h1> <p class="post-meta"> Created on February 26, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/cuda"> <i class="fa-solid fa-hashtag fa-sm"></i> CUDA</a>   ·   <a href="/blog/category/cuda"> <i class="fa-solid fa-tag fa-sm"></i> CUDA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <ul> <li><a href="https://github.com/HPC02/cuda_perf/blob/master/src/study_codes/tiled_mma_preview/cute_tiled_mma_preview.cu" rel="external nofollow noopener" target="_blank">cute_tiled_mma_preview.cu</a></li> </ul> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">// Configure data type.</span>
  <span class="k">using</span> <span class="n">TA</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">half_t</span><span class="p">;</span>
  <span class="k">using</span> <span class="n">TB</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">half_t</span><span class="p">;</span>
  <span class="k">using</span> <span class="n">TC</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">half_t</span><span class="p">;</span>

  <span class="c1">// Configure static "shared memory".</span>
  <span class="c1">// The "shared memory" is actually on host for preview purpose.</span>
  <span class="c1">// For tiled mma, the shared memory layout has to be static.</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">bM</span><span class="p">{</span><span class="mi">128</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">TA</span><span class="p">)};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">bN</span><span class="p">{</span><span class="mi">128</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">TB</span><span class="p">)};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">bK</span><span class="p">{</span><span class="mi">32</span><span class="p">};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">blk_M</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">bM</span><span class="o">&gt;</span><span class="p">{};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">blk_N</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">bN</span><span class="o">&gt;</span><span class="p">{};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">blk_K</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">bK</span><span class="o">&gt;</span><span class="p">{};</span>

  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_shape_A</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_shape</span><span class="p">(</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_K</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_shape_B</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_shape</span><span class="p">(</span><span class="n">blk_N</span><span class="p">,</span> <span class="n">blk_K</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_shape_C</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_shape</span><span class="p">(</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_stride_A</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_stride</span><span class="p">(</span><span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">blk_M</span><span class="p">)};</span>        <span class="c1">// Column-major</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_stride_B</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_stride</span><span class="p">(</span><span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">blk_N</span><span class="p">)};</span>        <span class="c1">// Column-major</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_stride_C</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_stride</span><span class="p">(</span><span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">blk_M</span><span class="p">)};</span>        <span class="c1">// Column-major</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_layout_A</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_layout</span><span class="p">(</span><span class="n">smem_shape_A</span><span class="p">,</span> <span class="n">smem_stride_A</span><span class="p">)};</span>  <span class="c1">// (blk_M, blk_K)</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_layout_B</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_layout</span><span class="p">(</span><span class="n">smem_shape_B</span><span class="p">,</span> <span class="n">smem_stride_B</span><span class="p">)};</span>  <span class="c1">// (blk_N, blk_K)</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">smem_layout_C</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_layout</span><span class="p">(</span><span class="n">smem_shape_C</span><span class="p">,</span> <span class="n">smem_stride_C</span><span class="p">)};</span>  <span class="c1">// (blk_M, blk_N)</span>

  <span class="k">auto</span> <span class="k">const</span> <span class="n">size_a</span><span class="p">{</span><span class="n">blk_M</span> <span class="o">*</span> <span class="n">blk_K</span><span class="p">};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">size_b</span><span class="p">{</span><span class="n">blk_N</span> <span class="o">*</span> <span class="n">blk_K</span><span class="p">};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">size_c</span><span class="p">{</span><span class="n">blk_M</span> <span class="o">*</span> <span class="n">blk_N</span><span class="p">};</span>

  <span class="k">auto</span> <span class="n">h_A</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">host_vector</span><span class="o">&lt;</span><span class="n">TA</span><span class="o">&gt;</span><span class="p">(</span><span class="n">size_a</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">h_B</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">host_vector</span><span class="o">&lt;</span><span class="n">TB</span><span class="o">&gt;</span><span class="p">(</span><span class="n">size_b</span><span class="p">);</span>
  <span class="k">auto</span> <span class="n">h_C</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">host_vector</span><span class="o">&lt;</span><span class="n">TC</span><span class="o">&gt;</span><span class="p">(</span><span class="n">size_c</span><span class="p">);</span>

  <span class="c1">// Make tensor for smem_A and smem_B.</span>
  <span class="k">auto</span> <span class="n">smem_tensor_A</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_tensor</span><span class="p">(</span><span class="n">h_A</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">smem_layout_A</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="n">smem_tensor_B</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_tensor</span><span class="p">(</span><span class="n">h_B</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">smem_layout_B</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="n">smem_tensor_C</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_tensor</span><span class="p">(</span><span class="n">h_C</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">smem_layout_C</span><span class="p">)};</span>
</code></pre></div></div> <h2 id="1-tiledmma-配置">1. TiledMMA 配置</h2> <p>位于 SMEM 中的 tile 大小为 $M \times N \times K = 128 \times 128 \times 32$，其中：</p> <ul> <li>A 矩阵为 $M \times K = 128 \times 32$，row-major layout；</li> <li>B 矩阵为 $K \times N = 32 \times 128$，column-major layout；</li> <li>C 矩阵为 $M \times N = 128 \times 128$。</li> </ul> <h3 id="11-mma_atom-配置">1.1. MMA_Atom 配置</h3> <p>MMA_Atom 使用的配置为 <strong>cute::SM80_16x8x16_F16F16F16F16_TN</strong>，使用一个 warp，即 32 个线程处理这个 MMA Atom。处理的 MNK 规模为：$M’ \times N’ \times K’ = 16 \times 8 \times 16$，其中：</p> <ul> <li>A sub-tile 为 $M’ \times K’ = 16 \times 16$；</li> <li>B sub-tile 为 $K’ \times N’ = 16 \times 8$；</li> <li>C sub-tile 为 $M’ \times N’ = 16 \times 8$。</li> </ul> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mma_atom
MMA_Atom
  ThrID:      _32:_1
  Shape_MNK:  (_16,_8,_16)
  LayoutA_TV: ((_4,_8),(_2,_2,_2)):((_32,_1),(_16,_8,_128))
  LayoutB_TV: ((_4,_8),(_2,_2)):((_16,_1),(_8,_64))
  LayoutC_TV: ((_4,_8),(_2,_2)):((_32,_1),(_16,_8))
</code></pre></div></div> <p>分配到线程，每个线程处理的元素数量为：A 矩阵为 2 x 2 x 2 = 8 个元素，B 矩阵为 2 x 2 = 4 个元素，得到 C 矩阵中 2 x 2 = 4 个元素。</p> <p><img src="/assets/images/cuda/20250226/tiled_mma_example/mma_atom_SM80_16x8x16_F16F16F16F16_TN.svg" alt="inverse_tv_layout_SM80_16x8x16_F16F16F16F16_TN"></p> <h3 id="12-tiled-mma-配置">1.2. Tiled MMA 配置</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">// Configure tiled MMA.</span>
  <span class="k">using</span> <span class="n">MmaTraits</span>           <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">MMA_Traits</span><span class="o">&lt;</span><span class="n">cute</span><span class="o">::</span><span class="n">SM80_16x8x16_F16F16F16F16_TN</span><span class="o">&gt;</span><span class="p">;</span>
  <span class="k">using</span> <span class="n">MmaAtomShape</span>        <span class="o">=</span> <span class="n">MmaTraits</span><span class="o">::</span><span class="n">Shape_MNK</span><span class="p">;</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">mma_atom</span>       <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">MMA_Atom</span><span class="o">&lt;</span><span class="n">MmaTraits</span><span class="o">&gt;</span><span class="p">{};</span>
  <span class="k">auto</span> <span class="k">const</span> <span class="n">mma_atom_shape</span> <span class="o">=</span> <span class="n">MmaAtomShape</span><span class="p">{};</span>
  <span class="c1">// Repeating the mma atom along the M, N, and K dimensions.</span>
  <span class="c1">// This increases the number of threads to process the tiled MMA.</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">MMA_LAYOUT_M</span><span class="p">{</span><span class="mi">2</span><span class="p">};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">MMA_LAYOUT_N</span><span class="p">{</span><span class="mi">2</span><span class="p">};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">MMA_LAYOUT_K</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
  <span class="k">auto</span> <span class="n">mma_layout</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_layout</span><span class="p">(</span>
    <span class="n">cute</span><span class="o">::</span><span class="n">make_shape</span><span class="p">(</span><span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">MMA_LAYOUT_M</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">MMA_LAYOUT_N</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">MMA_LAYOUT_K</span><span class="o">&gt;</span><span class="p">{}))};</span>
  <span class="c1">// Repeating the mma processing along the M, N, and K dimensions.</span>
  <span class="c1">// This does not increase the number of threads to process the tiled MMA.</span>
  <span class="c1">// But the number of registers required for processing the tiled MMA increases.</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_MMA_TILE_M</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_MMA_TILE_N</span><span class="p">{</span><span class="mi">2</span><span class="p">};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_MMA_TILE_K</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">MMA_TILE_M</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mma_atom_shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">MMA_LAYOUT_M</span> <span class="o">*</span> <span class="n">NUM_MMA_TILE_M</span><span class="p">};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">MMA_TILE_N</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mma_atom_shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">MMA_LAYOUT_N</span> <span class="o">*</span> <span class="n">NUM_MMA_TILE_N</span><span class="p">};</span>
  <span class="k">constexpr</span> <span class="kt">int</span> <span class="n">MMA_TILE_K</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">mma_atom_shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">MMA_LAYOUT_K</span> <span class="o">*</span> <span class="n">NUM_MMA_TILE_K</span><span class="p">};</span>
  <span class="k">auto</span> <span class="n">mma_tile</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_tile</span><span class="p">(</span><span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">MMA_TILE_M</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">MMA_TILE_N</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">cute</span><span class="o">::</span><span class="n">Int</span><span class="o">&lt;</span><span class="n">MMA_TILE_K</span><span class="o">&gt;</span><span class="p">{})};</span>
  <span class="k">auto</span> <span class="n">tiled_mma</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_tiled_mma</span><span class="p">(</span><span class="n">mma_atom</span><span class="p">,</span> <span class="n">mma_layout</span><span class="p">,</span> <span class="n">mma_tile</span><span class="p">)};</span>
</code></pre></div></div> <p>在 M 维度上，MMA Atom 重复 2 次，在 N 维度上重复 2 次，在 K 维度上重复 1 次。一共需要 2 x 2 x 1 = 4 个 MMA Atom 来处理这个 tiled MMA。每个 Atom 由一个 warp（32 个线程）处理，整个 tiled MMA 由 4 个 warp（128 个线程）处理。即 <strong>ThrLayoutVMNK = (_32,_2,_2,_1):(_1,_32,_64,_0)</strong>。经过此配置后，能处理的 MNK 规模为 $(M’ \times 2) \times (N’ \times 2) \times (K’ \times 1) = 32 \times 16 \times 16$。</p> <p>另外，通过配置 PermutationMNK（对应以前版本的 ValLayoutMNK），使得一个 tiled MMA 在 M/N/K 方向上处理更多的元素（即一个线程处理更多的元素）。这里配置 N 维度上乘以 2，得到该 tiled MMA 处理的 MNK 规模为 $32 \times 32 \times 16$，即 <strong>PermutationMNK: (_32,_32,_16)</strong>。</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tiled_mma
TiledMMA
  ThrLayoutVMNK:  (_32,_2,_2,_1):(_1,_32,_64,_0)
  PermutationMNK: (_32,_32,_16)
MMA_Atom
  ThrID:      _32:_1
  Shape_MNK:  (_16,_8,_16)
  LayoutA_TV: ((_4,_8),(_2,_2,_2)):((_32,_1),(_16,_8,_128))
  LayoutB_TV: ((_4,_8),(_2,_2)):((_16,_1),(_8,_64))
  LayoutC_TV: ((_4,_8),(_2,_2)):((_32,_1),(_16,_8))
</code></pre></div></div> <p><img src="/assets/images/cuda/20250226/tiled_mma_example/tiled_mma_SM80_16x8x16_F16F16F16F16_TN.svg" alt="tile_mma_SM80_16x8x16_F16F16F16F16_TN"></p> <h3 id="13-tiled-mma-划分总结">1.3. Tiled MMA 划分总结</h3> <p>以$M \times N \times K = 128 \times 128 \times 32$，以及 MMA Atom <strong>SM80_16x8x16_F16F16F16F16_TN</strong> 为例，MNK 三个维度分块划分（计算公式），可以按如下几种：</p> <ul> <li>$\frac{M}{M’} \times \frac{N}{N’} \times \frac{K}{K’}$ = $\frac{128}{16} \times \frac{128}{8} \times \frac{32}{16}$ = $8 \times 16 \times 2$ = $256$，即 MMA Atom 在 M 维度重复 8 次，N 维度 重复16 次，K 维度重复 2 次。Atom 总共需要循环 <strong>256</strong> 次。</li> <li>$\frac{M}{M’ \times MMA_LAYOUT_M} \times \frac{N}{N’ \times MMA_LAYOUT_N} \times \frac{K}{K’ \times MMA_LAYOUT_K}$ = $\frac{128}{16 \times 2} \times \frac{128}{8 \times 2} \times \frac{32}{16 \times 1}$ = $4 \times 8 \times 2$ = $64$，即 tiled MMA 在 M 维度重复 2 次，N 维度重复 2 次，K 维度重复 1 次。Atom 总共需要循环 <strong>64</strong> 次。</li> <li>其他配置方式，类似上面两种计算方式。</li> </ul> <p>一个 thread block 如何划分一个 tiled MMA，从性能上需要综合考虑以下几个因素：</p> <ul> <li>SM 上可用的寄存器数量，来确定一个 thread block 中可以有多少个线程来处理这个 tiled MMA。每个线程处理的元素数量越多，需要的寄存器数量就越多。</li> <li>在寄存器够用的情况下，尽量让一个 thread block 中的线程数量能够充分利用 SM 上的计算资源（即 CUDA Core、Tensor Core）。每个 tiled MMA 需要多少个线程来处理，取决于 MMA Atom 的配置以及 tiled MMA 的配置。</li> </ul> <h2 id="2-内存分块以及-tiledcopy">2. 内存分块以及 TiledCopy</h2> <p>在将 thread block 对应的 tile 内存再次分解为线程 sub-tile 过程中，使用 CuTe 分块，有三种方式：</p> <ul> <li>使用 partition 分块，得到 SMEM / GMEM slice。</li> <li>使用 partition_fragment 分块，得到寄存器片段（register fragment）。</li> <li>使用 TiledCopy / ldmatrix 进行分块以及传输。</li> </ul> <h3 id="21-使用-partitionpartition_fragment-分块">2.1. 使用 partition、partition_fragment 分块</h3> <p><strong>partition_A/B/C</strong></p> <p>使用 partition 方法，获取原始 layout 的分块，即生成一个 slice。分块之后，数据还是在 SMEM / GMEM 中，且保留了原有的 tile 的 stride 信息。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">auto</span> <span class="n">thread_mma</span><span class="p">{</span><span class="n">tiled_mma</span><span class="p">.</span><span class="n">get_slice</span><span class="p">(</span><span class="n">THREAD_IDX</span><span class="p">)};</span>

  <span class="k">auto</span> <span class="n">thread_layout_C_smem_tensor_A_no_tiled_copy</span><span class="p">{</span><span class="n">thread_mma</span><span class="p">.</span><span class="n">partition_A</span><span class="p">(</span><span class="n">smem_tensor_A</span><span class="p">)};</span>  <span class="c1">// (MMA, MMA_M, MMA_K)</span>
  <span class="k">auto</span> <span class="n">thread_layout_C_smem_tensor_B_no_tiled_copy</span><span class="p">{</span><span class="n">thread_mma</span><span class="p">.</span><span class="n">partition_B</span><span class="p">(</span><span class="n">smem_tensor_B</span><span class="p">)};</span>  <span class="c1">// (MMA, MMA_N, MMA_K)</span>
  <span class="k">auto</span> <span class="n">thread_layout_C_smem_tensor_C_no_tiled_copy</span><span class="p">{</span><span class="n">thread_mma</span><span class="p">.</span><span class="n">partition_C</span><span class="p">(</span><span class="n">smem_tensor_C</span><span class="p">)};</span>  <span class="c1">// (MMA, MMA_M, MMA_N)</span>
</code></pre></div></div> <p>打印信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>thread_layout_C_smem_tensor_A_no_tiled_copy
ptr[16b](0x5c6c073e78c0) o ((_2,_2,_2),_4,_2):((_128,_8,_1024),_32,_2048)
thread_layout_C_smem_tensor_B_no_tiled_copy
ptr[16b](0x5c6c073e98d0) o ((_2,_2),_8,_2):((_128,_1024),_16,_2048)
thread_layout_C_smem_tensor_C_no_tiled_copy
ptr[16b](0x5c6c073eb8e0) o ((_2,_2),_4,_8):((_128,_8),_32,_2048)
</code></pre></div></div> <p>其中，含义如下：</p> <table> <thead> <tr> <th>维度</th> <th>含义</th> </tr> </thead> <tbody> <tr> <td>MMA (第0维)</td> <td>一次 tiled MMA 计算中该线程负责的元素</td> </tr> <tr> <td>MMA_M (第1维)</td> <td>沿 M 方向需要循环的次数</td> </tr> <tr> <td>MMA_K (第2维)</td> <td>沿 K 方向需要循环的次数</td> </tr> </tbody> </table> <p>具体到 A/B/C 矩阵上，含义如下：</p> <table> <thead> <tr> <th>矩阵</th> <th>shape</th> <th>解释</th> </tr> </thead> <tbody> <tr> <td>A</td> <td>((_2,_2,_2), _4, _2)</td> <td>MMA=8个元素, M循环4次, K循环2次</td> </tr> <tr> <td>B</td> <td>((_2,_2), _8, _2)</td> <td>MMA=4个元素, N循环8次, K循环2次</td> </tr> <tr> <td>C</td> <td>((_2,_2), _4, _8)</td> <td>MMA=4个元素, M循环4次, N循环8次</td> </tr> </tbody> </table> <p>按线程切分之后，保留的第一个 mode，另外两个 mode 含义是（以 A 为例），MMA Atom 按 M 维度循环 4 次，K 维度循环 2 次。对于 B/C 矩阵类似。</p> <blockquote> <p>从打印信息看出，不论是直接使用 <strong>thread_layout_C_smem_tensor_A/B_no_tiled_copy</strong>，还是将其拷贝到寄存器，由于不连续，导致线程每次访问 2 _ 2 _ 2 = 8 个元素时，需要分开拷贝，即分 8 次访问 SMEM / GMEM 来加载数据到寄存器中。</p> </blockquote> <p><strong>partition_fragment_A/B/C</strong></p> <p>partition_fragment 则创建寄存器片段（register fragment），以复用寄存器数据。分块之后，数据在寄存器中，且不保留原有 tile 的 stride 信息，而是变为紧凑型布局。线程在做 gemm 之前，使用 copy 将数据从 SMEM 中加载到寄存器中。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">auto</span> <span class="n">thread_layout_C_register_tensor_A</span><span class="p">{</span><span class="n">thread_mma</span><span class="p">.</span><span class="n">partition_fragment_A</span><span class="p">(</span><span class="n">smem_tensor_A</span><span class="p">)};</span>  <span class="c1">// (MMA, MMA_M, MMA_K)</span>
  <span class="k">auto</span> <span class="n">thread_layout_C_register_tensor_B</span><span class="p">{</span><span class="n">thread_mma</span><span class="p">.</span><span class="n">partition_fragment_B</span><span class="p">(</span><span class="n">smem_tensor_B</span><span class="p">)};</span>  <span class="c1">// (MMA, MMA_N, MMA_K)</span>
  <span class="k">auto</span> <span class="n">thread_layout_C_register_tensor_C</span><span class="p">{</span><span class="n">thread_mma</span><span class="p">.</span><span class="n">partition_fragment_C</span><span class="p">(</span><span class="n">smem_tensor_C</span><span class="p">)};</span>  <span class="c1">// (MMA, MMA_M, MMA_N)</span>
</code></pre></div></div> <p>打印信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>thread_layout_C_register_tensor_A
ptr[16b](0x7ffc34e465f0) o ((_2,_2,_2),_4,_2):((_1,_2,_4),_8,_32)
thread_layout_C_register_tensor_B
ptr[16b](0x7ffc34e46670) o ((_2,_2),_8,_2):((_1,_2),_4,_32)
thread_layout_C_register_tensor_C
ptr[16b](0x7ffc34e466f0) o ((_2,_2),_4,_8):((_1,_2),_4,_16)
</code></pre></div></div> <p><strong>总结</strong></p> <p>使用 partition / partition_fragment 分块，分块数据可能不连续，导致需要多次访问 SMEM / GMEM。使用合适的 ldmatrix 可以一次将一次计算所需要的 sub-tile 数据拷贝到寄存器。</p> <h2 id="22-使用-tiledcopy--ldmatrix-进行分块以及传输">2.2. 使用 TiledCopy / ldmatrix 进行分块以及传输</h2> <p>前面使用的 partition / partition_fragment 方式，直接使用 TiledMMA/ThrMMA 分块得到的。使用 TiledCopy / ldmatrix 进行分块以及传输。设置 TiledCopy 需要的 Copy_Atom、CopyTraits（layout 信息），并使其传输的 sub-tile 大小与 tiled MMA 的计算需求一致。</p> <h3 id="221-copy-atom-配置">2.2.1. Copy Atom 配置</h3> <p>针对 A、B，使用 <strong>cute::SM75_U16x8_LDSM_T</strong> 生成 Copy Atom 来复制 A、B 的 sub-tile，其对应的 PTX 为：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ldmatrix</span><span class="p">.</span><span class="n">sync</span><span class="p">.</span><span class="n">aligned</span><span class="p">.</span><span class="n">m8n8</span><span class="p">.</span><span class="n">x4</span><span class="p">.</span><span class="n">trans</span><span class="p">.</span><span class="n">shared</span><span class="p">.</span><span class="n">b16</span> <span class="p">{</span><span class="n">r0</span><span class="p">,</span> <span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">,</span> <span class="n">r3</span><span class="p">},</span> <span class="p">[</span><span class="n">addr</span><span class="p">];</span>
</code></pre></div></div> <p>即使用一个 warp（32 个线程）同时加载一个 sub-tile 的数据到寄存器中。</p> <blockquote> <p>ldmatrix 只支持16位数据，不支持32位数据；另外 ldmatrix 是 PTX 指令，对应到 ncu 中看到的 LDSM 指令。x1 表示使用前 8 个线程，x2、x4 依次类推。m8n8 表示 load 操作的子块大小为 8x8，x4 则表示同时加载 4 个子块。</p> </blockquote> <p><strong>cute::SM75_U16x8_LDSM_T</strong> 打印信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>copy_atom_A
Copy_Atom
  ThrID:        _32:_1
  ValLayoutSrc: (_32,_8):(_8,_1)
  ValLayoutDst: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValLayoutRef: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValueType:    16b

copy_atom_B
Copy_Atom
  ThrID:        _32:_1
  ValLayoutSrc: (_32,_8):(_8,_1)
  ValLayoutDst: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValLayoutRef: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValueType:    16b
</code></pre></div></div> <p>在前面 TiledMMA 的配置中，经过 AtomLayout 以及 Permutation，最终得到的 tiled MMA 处理的 sub-tile 大小为 $M’ \times N’ \times K’ = 32 \times 32 \times 16$。其中 A 矩阵的 sub-tile 大小为 $M’ \times K’ = 32 \times 16$，B 矩阵的 sub-tile 大小为 $K’ \times N’ = 16 \times 32$。</p> <p>使用<strong>SM75_U16x8_LDSM_T</strong>，其一次拷贝的 sub-tile 大小为 $32 \times 8 = 256$，即对应上面打印信息中的 <strong>ValLayoutSrc: (_32,_8):(_8,_1)</strong>。因此，TiledCopy 的 Copy Atom 配置满足一次加载 执行一个 Tiled MMA 所需要的数据。</p> <p>另外，在 Tiled MMA 配置中，B 配置了 permutation，使得 B 矩阵的 sub-tile 大小与 A 矩阵大小一致，否则针对 B，就需要选择其他的 Copy Atom 来满足 tiled MMA 的计算需求。</p> <h3 id="222-tiledcopy-以及-threadcopy-创建">2.2.2. TiledCopy 以及 ThreadCopy 创建</h3> <p>创建 TiledCopy，Copy_Atom 配置如上<strong>SM75_U16x8_LDSM_T</strong>，TV-Layout 则由上述的 Tiled MMA 给出。代码如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">auto</span> <span class="n">copy_atom_A</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">Copy_Atom</span><span class="o">&lt;</span><span class="n">cute</span><span class="o">::</span><span class="n">SM75_U16x8_LDSM_T</span><span class="p">,</span> <span class="n">TA</span><span class="o">&gt;</span><span class="p">{};</span>
  <span class="k">auto</span> <span class="n">copy_atom_B</span> <span class="o">=</span> <span class="n">cute</span><span class="o">::</span><span class="n">Copy_Atom</span><span class="o">&lt;</span><span class="n">cute</span><span class="o">::</span><span class="n">SM75_U16x8_LDSM_T</span><span class="p">,</span> <span class="n">TB</span><span class="o">&gt;</span><span class="p">{};</span>

  <span class="k">auto</span> <span class="n">smem_tiled_copy_A</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_tiled_copy_A</span><span class="p">(</span><span class="n">copy_atom_A</span><span class="p">,</span> <span class="n">tiled_mma</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="n">smem_tiled_copy_B</span><span class="p">{</span><span class="n">cute</span><span class="o">::</span><span class="n">make_tiled_copy_B</span><span class="p">(</span><span class="n">copy_atom_B</span><span class="p">,</span> <span class="n">tiled_mma</span><span class="p">)};</span>
</code></pre></div></div> <p>Tiled Copy 信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>smem_tiled_copy_A
TiledCopy
  Tiler_MN:       (_32,_16)
  TiledLayout_TV: ((_4,_8,_2,_2),((_2,_2,_2),(_1,_1))):((_64,_1,_16,_0),((_32,_8,_256),(_0,_0)))
Copy_Atom
  ThrID:        _32:_1
  ValLayoutSrc: (_32,_8):(_8,_1)
  ValLayoutDst: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValLayoutRef: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValueType:    16b

smem_tiled_copy_B
TiledCopy
  Tiler_MN:       (_32,_16)
  TiledLayout_TV: ((_4,_8,_2,_2),((_2,_2),(_2,_1))):((_64,_1,_0,_8),((_32,_256),(_16,_0)))
Copy_Atom
  ThrID:        _32:_1
  ValLayoutSrc: (_32,_8):(_8,_1)
  ValLayoutDst: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValLayoutRef: ((_4,_8),(_1,_2,_4)):((_16,_1),(_1,_8,_64))
  ValueType:    16b
</code></pre></div></div> <p><strong>使用 Thread Copy 以及创建本线程 sub-tile</strong></p> <p>分为两步，首先使用 <strong>ThrCopy&lt;…&gt;::partition_S</strong> 获取线程的 tensor，再使用 <strong>ThrCopy&lt;…&gt;::retile_D</strong> 获取调整 shape 之后的 tensor。代码如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">auto</span> <span class="n">smem_thread_copy_A</span><span class="p">{</span><span class="n">smem_tiled_copy_A</span><span class="p">.</span><span class="n">get_slice</span><span class="p">(</span><span class="n">THREAD_IDX</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="n">smem_thread_copy_B</span><span class="p">{</span><span class="n">smem_tiled_copy_B</span><span class="p">.</span><span class="n">get_slice</span><span class="p">(</span><span class="n">THREAD_IDX</span><span class="p">)};</span>

  <span class="k">auto</span> <span class="n">thread_layout_C_smem_tensor_A_tiled_copy</span><span class="p">{</span><span class="n">smem_thread_copy_A</span><span class="p">.</span><span class="n">partition_S</span><span class="p">(</span><span class="n">smem_tensor_A</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="n">thread_layout_C_smem_tensor_B_tiled_copy</span><span class="p">{</span><span class="n">smem_thread_copy_B</span><span class="p">.</span><span class="n">partition_S</span><span class="p">(</span><span class="n">smem_tensor_B</span><span class="p">)};</span>

  <span class="k">auto</span> <span class="n">thread_layout_C_register_tensor_A_copy_view</span><span class="p">{</span><span class="n">smem_thread_copy_A</span><span class="p">.</span><span class="n">retile_D</span><span class="p">(</span><span class="n">thread_layout_C_register_tensor_A</span><span class="p">)};</span>
  <span class="k">auto</span> <span class="n">thread_layout_C_register_tensor_B_copy_view</span><span class="p">{</span><span class="n">smem_thread_copy_B</span><span class="p">.</span><span class="n">retile_D</span><span class="p">(</span><span class="n">thread_layout_C_register_tensor_B</span><span class="p">)};</span>
</code></pre></div></div> <p><strong>thread_layout_C_smem_tensor_A/B_tiled_copy</strong> 作为源 tensor，他们的信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>thread_layout_C_smem_tensor_A_tiled_copy
ptr[16b](0x57b7b93248c0) o ((_8,_1),_4,_2):((_1,_0),_32,_2048)
thread_layout_C_smem_tensor_B_tiled_copy
ptr[16b](0x57b7b93268d0) o ((_8,_1),_4,_2):((_1,_0),_32,_2048)
</code></pre></div></div> <p>作为目的 tensor 的 <strong>thread_layout_C_register_tensor_A/B</strong>（由 <strong>ThrMMA&lt;…&gt;::partition_fragment_A/B</strong> 获取），其信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>thread_layout_C_register_tensor_A
ptr[16b](0x7ffc34e465f0) o ((_2,_2,_2),_4,_2):((_1,_2,_4),_8,_32)
thread_layout_C_register_tensor_B
ptr[16b](0x7ffc34e46670) o ((_2,_2),_8,_2):((_1,_2),_4,_32)
</code></pre></div></div> <p>所以需要对目的 tensor 进行调整，使得其 shape 与源 tensor 一致。调整之后的目的 tensor <strong>thread_layout_C_register_tensor_A/B_copy_view</strong> 的信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>thread_layout_C_register_tensor_A_copy_view
ptr[16b](0x7ffd7a129350) o ((_8,_1),_4,_2):((_1,_0),_8,_32)
thread_layout_C_register_tensor_B_copy_view
ptr[16b](0x7ffd7a1293d0) o ((_8,_1),_4,_2):((_1,_0),_8,_32)
</code></pre></div></div> <h2 id="参考及资料">参考及资料</h2> <ul> <li> <a href="https://leimao.github.io/blog/CuTe-Tiled-MMA/" rel="external nofollow noopener" target="_blank">CuTe Tiled MMA</a>：Mao Lei 博客</li> <li> <a href="https://github.com/leimao/CUTLASS-Examples/tree/main/examples/cute_general_matrix_multiplication" rel="external nofollow noopener" target="_blank">CuTe General Matrix Multiplication</a>：Mao Lei GitHub GEMM 代码</li> <li> <a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md" rel="external nofollow noopener" target="_blank">0t_mma_atom</a>：CUTLASS-CuTe 0t_mma_atom 文档 <strong>待阅读</strong> </li> <li> <a href="https://leimao.github.io/blog/CuTe-ldmatrix/" rel="external nofollow noopener" target="_blank">CuTe ldmatrix</a>：Mao Lei 博客 ldmatrix <strong>待阅读</strong> </li> <li><a href="https://zhuanlan.zhihu.com/p/697228676" rel="external nofollow noopener" target="_blank">ldmatrix指令例子和其带来的smem bank冲突</a></li> <li><a href="https://zhuanlan.zhihu.com/p/696231622" rel="external nofollow noopener" target="_blank">ldmatrix与swizzle（笔记）</a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/al-folio-local-deploy-ubuntu2404/">al-folio 本地部署记录（Ubuntu 24.04）</a> </li> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Roderick Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>