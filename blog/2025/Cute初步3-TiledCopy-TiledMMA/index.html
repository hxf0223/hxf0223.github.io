<!DOCTYPE html> <html lang="zh-CN"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CUTLASS-Cute 初步(3)：TiledCopy 以及 TiledMMA | Roderick Huang </title> <meta name="author" content="Roderick Huang"> <meta name="description" content="Roderick Huang 的个人博客，记录工作与技术。 "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="manifest" href="/manifest.json"> <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff"> <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1a1a2e"> <script>
  if ('serviceWorker' in navigator) {
    window.addEventListener('load', function () {
      navigator.serviceWorker.register('/sw.js');
    });
  }
</script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:; worker-src 'self';"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hxf0223.github.io/blog/2025/Cute%E5%88%9D%E6%AD%A53-TiledCopy-TiledMMA/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Roderick</span> Huang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">CUTLASS-Cute 初步(3)：TiledCopy 以及 TiledMMA</h1> <p class="post-meta"> Created on February 26, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/cuda"> <i class="fa-solid fa-hashtag fa-sm"></i> CUDA</a>   ·   <a href="/blog/category/cuda"> <i class="fa-solid fa-tag fa-sm"></i> CUDA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <ul> <li> <a href="https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/tiled_copy.cu" rel="external nofollow noopener" target="_blank">tiled_copy.cu</a>：官方示例</li> </ul> <h2 id="1-cute-tiledcopy">1. Cute TiledCopy</h2> <p>层次化的 copy 抽象，将 Copy_Atom 分为几个可组合的层次：</p> <ul> <li>CopyOperation：NVidia在不同的硬件架构、不同的存储层次之间数据搬运提供了不同的指令，如 ldmatrix 和 LDS 等，还有针对Ampere架构的 cp.async 等。</li> <li>Copy_Traits：和 MMA_Traits 类似，提供了 CopyOperation 类型没有提供，但是其使用者 Copy_Atom 却需要的起到桥梁作用的信息；</li> <li>Copy_Atom：封装了基本的拷贝指令，针对 SRC-DST 的一次搬运；</li> </ul> <p>TiledCopy 则根据提供的 LayoutCopy_TV 执行 Copy_Atom，可能需要重复多次的 atom 搬运操作。</p> <h3 id="11-copy_atom">1.1. Copy_Atom</h3> <p>Copy_Atom 封装基本的拷贝指令，所以叫 Atom，即针对 SRC-DST 的一次搬运。适配不同的硬件指令集，比如通用拷贝/向量化 UniversalCopy&lt;…&gt;，cp.async（Ampere架构）。</p> <p><img src="/assets/images/cuda/20250226/cute_tiled_copy/Copy_Atom_Structure.png" alt="copy_atom_structure"></p> <p>需要关注的两个模板参数是：CopyOperation 和 Copy_Traits。CopyOperation 定义了具体的拷贝指令，而 Copy_Traits 定义了拷贝的元信息，比如每次拷贝多少个元素（元素类型），SRC-DST 的 Layout 等。不同平台，实现不同的 Copy_Traits。（个人理解：一些 copy 操作也需要用到 layout 信息，以及 bits 位宽等信息）</p> <p><img src="/assets/images/cuda/20250226/cute_tiled_copy/Copy_Traits_Arch.png" alt="copy_traits_arch"></p> <p>部分代码实现文件列表：</p> <ul> <li><a href="https://github.com/NVIDIA/cutlass/blob/v4/include/cute/atom/copy_atom.hpp#L54" rel="external nofollow noopener" target="_blank">cute/atom/copy_atom.hpp – Copy_Atom</a></li> <li><a href="https://github.com/NVIDIA/cutlass/blob/v4/include/cute/atom/copy_traits.hpp#L113" rel="external nofollow noopener" target="_blank">cute/atom/copy_traits.hpp – copy_unpack</a></li> <li><a href="https://github.com/NVIDIA/cutlass/blob/v4/include/cute/atom/copy_traits.hpp#L66" rel="external nofollow noopener" target="_blank">cute/atom/copy_traits.hpp – Copy_Traits</a></li> <li> <a href="https://github.com/NVIDIA/cutlass/blob/v4/include/cute/arch/copy_sm90.hpp" rel="external nofollow noopener" target="_blank">cute/arch/copy_sm90.hpp</a>：针对 SM90 架构的 Copy_Traits 实现</li> </ul> <h3 id="12-tiledcopy">1.2. TiledCopy</h3> <p>TiledCopy 封装 Copy_Atom，根据 LayoutCopy_TV 执行 Copy_Atom，可能需要重复多次的 atom 搬运操作。其 template 参数有：</p> <ul> <li>LayoutCopy_TV：定义 Thread Layout，以及 Value Layout；</li> <li>ShapeTiler_MN：切分器的 shape；</li> <li>Copy_Atom：定义复制指令；</li> </ul> <p>ThrCopy 完成实际的生成线程对应的 tensor（软件工程功能划分需要，剥离出来的功能模块）。</p> <h3 id="13-make_tiled_copy">1.3. make_tiled_copy</h3> <p>提供工厂函数，提供 thr_layout、val_layout、CopyOperation 参数生成 TiledCopy 实例。其中，thr_layout、val_layout 分别定义线程划分 layout 和每个线程拷贝数据的 layout。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">make_tiled_copy</span><span class="p">(</span><span class="n">copy_atom</span><span class="p">,</span> <span class="n">thr_layout</span><span class="p">,</span> <span class="n">val_layout</span><span class="p">)</span>
</code></pre></div></div> <h3 id="14-copy-执行">1.4. copy 执行</h3> <p>copy 函数是拷贝的实际执行函数，完成线程指令的执行：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">copy</span><span class="p">(</span><span class="n">TiledCopy</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">copy</span><span class="p">,</span> <span class="n">Tensor</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">src</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">dst</span><span class="p">);</span>
<span class="kt">void</span> <span class="nf">copy_if</span><span class="p">(</span><span class="n">TiledCopy</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">copy</span><span class="p">,</span> <span class="n">PrdTensor</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">pred</span><span class="p">,</span> <span class="n">Tensor</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">src</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">dst</span><span class="p">);</span>
</code></pre></div></div> <h3 id="15-可视化工具">1.5. 可视化工具</h3> <ul> <li><a href="https://github.com/flashinfer-ai/cutlass-viz" rel="external nofollow noopener" target="_blank">cutlass-viz</a></li> <li><a href="https://github.com/hxf0223/cute_render" rel="external nofollow noopener" target="_blank">cute_render</a></li> <li><a href="https://github.com/NTT123/cute-viz" rel="external nofollow noopener" target="_blank">cute-viz</a></li> </ul> <h2 id="2-mmaatom-以及-tiledmma">2. MMAAtom 以及 TiledMMA</h2> <p>分块 MMA 抽象，将 MMA_Atom 分为几个可组合的层次：</p> <ul> <li>MMAOperation：封装 D=A*B + C 的指令封装，以使用不同的数据类型以及 PTX 指令，包括使用 CUDA Core / Tensor Core。如 UniversalFMA&lt;&gt;、SM80_16x8x8_F32F16F16F32_TN。</li> <li>MMA_Traits：和 Copy_Traits 类似，提供了 MMAOperation 类型没有提供，但是其使用者 MMA_Atom 却需要的起到桥梁作用的信息。如数据类型信息，TV layout 信息。</li> <li>MMA_Atom：将 MMAOperation 和 MMA_Traits 结合，并提供 fragment 划分接口。</li> <li>TiledMMA：根据 LayoutTile_TV 切分的线程布局，重复使用 MMA_Atom 完成分块矩阵乘加计算。</li> <li>ThrMMA：完成实际的生成线程对应的 tensor。</li> </ul> <h3 id="21-mmaoperation">2.1. MMAOperation</h3> <p>以<strong>SM80_16x8x8_F32F16F16F32_TN</strong>为例，封装了 SM80 架构下，16x8x8 大小的矩阵乘加指令 <strong>D=A * B + C</strong>，数据类型为 A:F16、B:F16、C:F32、D:F32。A 矩阵 row-major，B 矩阵 column-major。</p> <blockquote> <p>BLAS 中约定 normal 矩阵为列优先。T(transpose) 表示使用转置矩阵，即 row-major 存储。 下图原图见 Thakkar_BLISRetreat2023.pdf 第 30 页。</p> </blockquote> <p><strong>SM80_16x8x8_F32F16F16F32_TN</strong> 对应的 inverse TV-Layout 如下：</p> <p><img src="/assets/images/cuda/20250226/cute_tiled_mma/abc_layout_SM80_16x8x8_F32F16F16F32_TN.png" alt="SM80_16x8x8_F32F16F16F32_TN"></p> <blockquote> <p>inverse TV-Layout 表示 <strong>element coordinate -&gt; thread index</strong>的映射关系。</p> </blockquote> <p><strong>SM80_16x8x8_F32F16F16F32_TN</strong> 对应的 MMA_Atom 信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MMA_Atom
  ThrID:      _32:_1
  Shape_MNK:  (_16,_8,_8)
  LayoutA_TV: ((_4,_8),(_2,_2)):((_32,_1),(_16,_8))
  LayoutB_TV: ((_4,_8),_2):((_16,_1),_8)
  LayoutC_TV: ((_4,_8),(_2,_2)):((_32,_1),(_16,_8))
</code></pre></div></div> <p>对应的代码如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">print</span><span class="p">(</span><span class="n">MMA_Atom</span><span class="o">&lt;</span><span class="n">SM80_16x8x8_F32F16F16F32_TN</span><span class="o">&gt;</span><span class="p">{});</span>
<span class="c1">// using MMA = MMA_Traits&lt;SM80_16x8x8_F32F16F16F32_TN&gt;;</span>
<span class="c1">// print("ALayout: "), print(typename MMA::ALayout{}), print("\n");</span>
<span class="c1">// print("BLayout: "), print(typename MMA::BLayout{}), print("\n");</span>
<span class="c1">// print("CLayout: "), print(typename MMA::CLayout{}), print("\n");</span>

<span class="n">MMA_Atom</span><span class="o">&lt;</span><span class="n">SM80_16x8x8_F32F16F16F32_TN</span><span class="o">&gt;</span> <span class="n">mma</span><span class="p">;</span>
<span class="n">print_latex</span><span class="p">(</span><span class="n">mma</span><span class="p">);</span>

<span class="cm">/* 或者如下写法
TiledMMA tiled_mma = make_tiled_mma(SM80_8x8x4_F64F64F64F64_TN{});
print_latex(tiled_mma);
*/</span>
</code></pre></div></div> <ul> <li><strong>TODO: SM80_16x8x8_F32F16F16F32_TN 一条指令处理几个数据？</strong></li> <li><strong>TODO：Tensor Core 的指令是什么，对应的布局是什么规则？</strong></li> </ul> <p>CUDA PTX 文档也给出了指令 m16n8k8 的布局信息：<a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-1688" rel="external nofollow noopener" target="_blank">9.7.14.5.7. Matrix Fragments for mma.m16n8k8</a>。</p> <h3 id="22-mma_traits">2.2. MMA_Traits</h3> <p>MMA_Traits 提供数据类型信息，以及 TV layout 信息，比如需要根据 MMAOperation 中定义的指令，补充 A/B/C 的 layout 信息。需要提供的信息如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">ElementDVal</span> <span class="o">=</span>  <span class="c1">// Logical A-value type</span>
<span class="k">using</span> <span class="n">ElementAVal</span> <span class="o">=</span>  <span class="c1">// Logical B-value type</span>
<span class="k">using</span> <span class="n">ElementBVal</span> <span class="o">=</span>  <span class="c1">// Logical C-value type</span>
<span class="k">using</span> <span class="n">ElementCVal</span> <span class="o">=</span>  <span class="c1">// Logical D-value type</span>

<span class="k">using</span> <span class="n">ElementAFrg</span> <span class="o">=</span>  <span class="c1">// A-type consumed by MMA  (if ommitted, same as ElementAVal)</span>
<span class="k">using</span> <span class="n">ElementBFrg</span> <span class="o">=</span>  <span class="c1">// B_type consumed by MMA  (if ommitted, same as ElementBVal)</span>
<span class="k">using</span> <span class="n">ElementCFrg</span> <span class="o">=</span>  <span class="c1">// C_type consumed by MMA  (if ommitted, same as ElementCVal)</span>

<span class="k">using</span> <span class="n">Shape_MNK</span> <span class="o">=</span>    <span class="c1">// Logical MxNxK shape of the MMA</span>

<span class="k">using</span> <span class="n">ThrID</span>     <span class="o">=</span>    <span class="c1">// Logical thread id (tid) -&gt; tidx</span>

<span class="k">using</span> <span class="n">ALayout</span> <span class="o">=</span>      <span class="c1">// (Logical thread id (tid), Logical value id (vid)) -&gt; Flat MK-coord</span>
<span class="k">using</span> <span class="n">BLayout</span> <span class="o">=</span>      <span class="c1">// (Logical thread id (tid), Logical value id (vid)) -&gt; Flat NK-coord</span>
<span class="k">using</span> <span class="n">CLayout</span> <span class="o">=</span>      <span class="c1">// (Logical thread id (tid), Logical value id (vid)) -&gt; Flat MN-coord</span>
</code></pre></div></div> <h3 id="23-mma_atom">2.3. MMA_Atom</h3> <p>MMA_Atom 封装了 MMAOperation 和 MMA_Traits。</p> <p><strong>创建寄存器 fragment</strong></p> <p>提供了创建寄存器 fragment 的接口 make_fragment_A/B/C：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">CTensor</span><span class="p">&gt;</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="nf">make_fragment_C</span><span class="p">(</span><span class="n">CTensor</span><span class="o">&amp;&amp;</span> <span class="n">ctensor</span><span class="p">);</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">ATensor</span><span class="p">&gt;</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="nf">make_fragment_A</span><span class="p">(</span><span class="n">ATensor</span><span class="o">&amp;&amp;</span> <span class="n">atensor</span><span class="p">);</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">BTensor</span><span class="p">&gt;</span>
<span class="k">static</span> <span class="k">constexpr</span> <span class="k">auto</span> <span class="nf">make_fragment_B</span><span class="p">(</span><span class="n">BTensor</span><span class="o">&amp;&amp;</span> <span class="n">btensor</span><span class="p">);</span>
</code></pre></div></div> <p><strong>调用 FMA 指令</strong></p> <p>提供 call 接口，调用 MMAOperation 指令：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TD</span><span class="p">,</span> <span class="k">class</span> <span class="nc">DLayout</span><span class="p">,</span>
            <span class="k">class</span> <span class="nc">TA</span><span class="p">,</span> <span class="k">class</span> <span class="nc">ALayout</span><span class="p">,</span>
            <span class="k">class</span> <span class="nc">TB</span><span class="p">,</span> <span class="k">class</span> <span class="nc">BLayout</span><span class="p">,</span>
            <span class="k">class</span> <span class="nc">TC</span><span class="p">,</span> <span class="k">class</span> <span class="nc">CLayout</span><span class="p">&gt;</span>
<span class="k">constexpr</span> <span class="kt">void</span> <span class="n">call</span><span class="p">(</span><span class="n">Tensor</span><span class="o">&lt;</span><span class="n">TD</span><span class="p">,</span> <span class="n">DLayout</span><span class="o">&gt;&amp;</span> <span class="n">D</span><span class="p">,</span>
       <span class="n">Tensor</span><span class="o">&lt;</span><span class="n">TA</span><span class="p">,</span> <span class="n">ALayout</span><span class="o">&gt;</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">A</span><span class="p">,</span>
       <span class="n">Tensor</span><span class="o">&lt;</span><span class="n">TB</span><span class="p">,</span> <span class="n">BLayout</span><span class="o">&gt;</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">B</span><span class="p">,</span>
       <span class="n">Tensor</span><span class="o">&lt;</span><span class="n">TC</span><span class="p">,</span> <span class="n">CLayout</span><span class="o">&gt;</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">C</span><span class="p">)</span> <span class="k">const</span><span class="p">;</span>
</code></pre></div></div> <ul> <li>TODO: 调用之前进行一个 unpack 操作？</li> </ul> <h3 id="24-tiledmma">2.4. TiledMMA</h3> <p>TiledMMA 的模版参数表达了 TiledMMA 在 MMA_Atom 上的扩展逻辑：AtomLayoutMNK 表示 M、N、K 方向上分别重复几次 Atom，这种重复会要求更多的执行线程。get_slice、get_thread_slice 函数功过给定线程 id 则获取线程对应到 ThrMMA 结构。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">MMA_Atom</span><span class="p">,</span>
          <span class="k">class</span> <span class="nc">AtomLayoutMNK</span><span class="p">,</span>
          <span class="k">class</span> <span class="nc">PermutationMNK</span> <span class="o">=</span> <span class="n">Tile</span><span class="o">&lt;</span><span class="n">Underscore</span><span class="p">,</span><span class="n">Underscore</span><span class="p">,</span><span class="n">Underscore</span><span class="p">&gt;</span><span class="o">&gt;</span>
<span class="k">struct</span> <span class="nc">TiledMMA</span> <span class="o">:</span> <span class="n">MMA_Atom</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">get_slice</span><span class="p">(</span><span class="n">ThrIdx</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">thr_idx</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">thr_vmnk</span> <span class="o">=</span> <span class="n">thr_layout_vmnk_</span><span class="p">.</span><span class="n">get_flat_coord</span><span class="p">(</span><span class="n">thr_idx</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">ThrMMA</span><span class="o">&lt;</span><span class="n">TiledMMA</span><span class="p">,</span> <span class="k">decltype</span><span class="p">(</span><span class="n">thr_vmnk</span><span class="p">)</span><span class="o">&gt;</span><span class="p">{</span><span class="o">*</span><span class="k">this</span><span class="p">,</span> <span class="n">thr_vmnk</span><span class="p">};</span>
  <span class="p">}</span>

  <span class="k">auto</span> <span class="nf">get_thread_slice</span><span class="p">(</span><span class="n">ThrIdx</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">thr_idx</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">get_slice</span><span class="p">(</span><span class="n">thr_idx</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">auto</span> <span class="n">thrfrg_C</span><span class="p">(</span><span class="n">CTensor</span><span class="o">&amp;&amp;</span> <span class="n">ctensor</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
    <span class="p">....</span>
  <span class="p">}</span>
<span class="p">};</span>
</code></pre></div></div> <p>三个模板参数的含义：</p> <table> <thead> <tr> <th>参数名</th> <th>类型</th> <th>说明</th> </tr> </thead> <tbody> <tr> <td>MMA_Atom</td> <td>底层指令</td> <td>定义单条 MMA 指令涉及的线程和值的布局</td> </tr> <tr> <td>AtomLayoutMNK</td> <td><code class="language-plaintext highlighter-rouge">Layout&lt;Shape&lt;_2,_2,_1&gt;&gt;</code></td> <td>在 M/N/K 方向上重复多少个 atom（分配更多线程）</td> </tr> <tr> <td>PermutationMNK</td> <td><code class="language-plaintext highlighter-rouge">Layout&lt;Shape&lt;_1,_2,_1&gt;&gt;</code></td> <td>每个线程在 M/N/K 方向上处理更多的值（不增加线程）</td> </tr> </tbody> </table> <blockquote> <p>AtomLayoutMNK 决定如何将 MMA_Atom 复制到更多的线程上执行，且将 MMA_ATOM 处理的线程扩展为 size(AtomLayoutMNK) 倍数。PermutationMNK 决定每个线程如何处理更多的值（即哪些逻辑坐标位置的值）。</p> </blockquote> <blockquote> <p>3.4 之前版本还有 ValLayoutMNK 参数，从 3.4 版本开始 去掉该模板参数。PermutationMNK 可以替代 ValLayoutMNK 的功能。即 AtomLayoutMNK 定义 Thread 扩展，PermutationMNK 定义 Value 级别的扩展，即执行多次 Atom，<strong>使用 PermutationMNK 导致线程需要占用更多的寄存器</strong>。</p> </blockquote> <blockquote> <p>PermutationMNK 的展开讲述见下面章节。</p> </blockquote> <h4 id="241-四层-layout-以及获取线程-fragment">2.4.1. 四层 Layout 以及获取线程 fragment</h4> <p>根据给定的 MMA_Atom、以及 AtomLayoutMNK 参数，生成一个四层 Layout 结构：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">ThrLayoutVMNK</span> <span class="o">=</span> <span class="k">decltype</span><span class="p">(</span><span class="n">tiled_product</span><span class="p">(</span><span class="n">AtomThrID</span><span class="p">{},</span> <span class="n">AtomLayoutMNK</span><span class="p">{}));</span>
<span class="n">ThrLayoutVMNK</span> <span class="n">thr_layout_vmnk_</span><span class="p">;</span>
</code></pre></div></div> <ul> <li>Mode 0 (V): Threads within a single atom</li> <li>Mode 1 (M): Atom tiles in M dimension</li> <li>Mode 2 (N): Atom tiles in N dimension</li> <li>Mode 3 (K): Atom tiles in K dimension</li> </ul> <p>以 thrfrg_A 为例：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Tile a tensor or a layout from shape</span>
  <span class="c1">//   (M,K,...)</span>
  <span class="c1">// to shape</span>
  <span class="c1">//   ((ThrV,(ThrM,ThrK)),(FrgV,(RestM,RestK,...)))</span>
  <span class="c1">// where</span>
  <span class="c1">//   ThrV: The threads local to an MMA. layout&lt;0&gt;(ThrLayoutVMNK): ThrV -&gt; thread_idx</span>
  <span class="c1">//   ThrM: The threads tiled in M.      layout&lt;1&gt;(ThrLayoutVMNK): ThrM -&gt; thread_idx</span>
  <span class="c1">//   ThrK: The threads tiled in K.      layout&lt;3&gt;(ThrLayoutVMNK): ThrK -&gt; thread_idx</span>
  <span class="c1">//   FrgV:  The values local to an MMA.</span>
  <span class="c1">//   RestM: The values tiled in M.</span>
  <span class="c1">//   RestK: The values tiled in K.</span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">ATensor</span><span class="p">&gt;</span>
<span class="k">constexpr</span> <span class="k">auto</span> <span class="n">thrfrg_A</span><span class="p">(</span><span class="n">ATensor</span><span class="o">&amp;&amp;</span> <span class="n">atensor</span><span class="p">)</span> <span class="k">const</span><span class="p">;</span>
</code></pre></div></div> <p>即得到的线程切分后的 subtile 布局为 <strong>((ThrV,(ThrM,ThrK)),(FrgV,(RestM,RestK,…)))</strong>。</p> <h3 id="25-thrmma">2.5. ThrMMA</h3> <p>TiledMMA 根据具体的线程 id 分解得到 ThrMMA 结构，提供 partition 函数接口，以及 partition_fragment 函数接口。</p> <p>如 Tensor C 为 BLK_M x BLK_N，则 partition_C 可以得到线程级别的任务，维度为 (MMA, MMA_M, MMA_N), MMA 表达了 TileMMA 一次能计算的单元，MMA_M, MMA_N 表达了 M 方向和 N 方向需要分块数量。</p> <p>partition_fragment 类函数是按照 partition 类函数返回的 Tensor 形状生成的对应的寄存器表示。</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">TiledMMA</span><span class="p">,</span> <span class="k">class</span> <span class="nc">ThrVMNK</span><span class="p">&gt;</span>
<span class="k">struct</span> <span class="nc">ThrMMA</span> <span class="o">:</span> <span class="n">TiledMMA</span> <span class="p">{</span>
  <span class="n">Tensor</span> <span class="n">partition_C</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">C</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="n">partition_A</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">A</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="n">partition_B</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">B</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="n">partition_fragment_C</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">C</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="n">partition_fragment_A</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">A</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="n">partition_fragment_B</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">B</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="26-permutation置换">2.6. Permutation：置换</h3> <p>Permutation 是一个 Tiler，由三个独立的分量组成，分别作用于 M、N、K 维度。它在 TV-layout 分配之前，对逻辑坐标进行重新映射。以 SM80_8x8x4_F64F64F64F64_TN 为例，其 inverse TV-Layout 如下：</p> <p><img src="/assets/images/cuda/20250226/cute_tiled_mma/abc_SM80_8x8x4_F64F64F64F64_TN.webp" alt="SM80_8x8x4_F64F64F64F64_TN"></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TiledMMA
  ThrLayoutVMNK:  (_32,_1,_1,_1):(_1,_0,_0,_0)
  PermutationMNK: (_,_,_)
MMA_Atom
  ThrID:      _32:_1
  Shape_MNK:  (_8,_8,_4)
  LayoutA_TV: ((_4,_8),_1):((_8,_1),_0)
  LayoutB_TV: ((_4,_8),_1):((_8,_1),_0)
  LayoutC_TV: ((_4,_8),_2):((_16,_1),_8)
</code></pre></div></div> <p>代码如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TiledMMA</span> <span class="n">tiled_mma</span> <span class="o">=</span> <span class="n">make_tiled_mma</span><span class="p">(</span><span class="n">SM80_8x8x4_F64F64F64F64_TN</span><span class="p">{});</span>
<span class="cm">/* TiledMMA tiled_mma = make_tiled_mma(SM80_8x8x4_F64F64F64F64_TN{}, Layout&lt;Shape&lt;_1, _1, _1&gt;&gt;{}, Tile&lt;_8, _8, _4&gt;{}); */</span>

<span class="n">print</span><span class="p">(</span><span class="n">tiled_mma</span><span class="p">),</span> <span class="n">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="cm">/*print("ALayout: "), print(typename decltype(tiled_mma)::ALayout{}), print("\n");
print("BLayout: "), print(typename decltype(tiled_mma)::BLayout{}), print("\n");
print("CLayout: "), print(typename decltype(tiled_mma)::CLayout{}), print("\n");*/</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"</span><span class="se">\n</span><span class="s">MMA Atom Layout:"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">print_latex</span><span class="p">(</span><span class="n">tiled_mma</span><span class="p">);</span>
</code></pre></div></div> <p>使用 permutation 参数将线程处理的单元 size 修改为 8x16x8，即 N、K 方向扩大为两倍，M 方向不变：</p> <p><img src="/assets/images/cuda/20250226/cute_tiled_mma/abc_SM80_8x8x4_F64F64F64F64_TN_permute_8_16_8.webp" alt="Permutation 8x16x8"></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TiledMMA
  ThrLayoutVMNK:  (_32,_1,_1,_1):(_1,_0,_0,_0)
  PermutationMNK: (_8,_16,_8)
MMA_Atom
  ThrID:      _32:_1
  Shape_MNK:  (_8,_8,_4)
  LayoutA_TV: ((_4,_8),_1):((_8,_1),_0)
  LayoutB_TV: ((_4,_8),_1):((_8,_1),_0)
  LayoutC_TV: ((_4,_8),_2):((_16,_1),_8)
</code></pre></div></div> <p>代码如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TiledMMA</span> <span class="n">tiled_mma</span> <span class="o">=</span> <span class="n">make_tiled_mma</span><span class="p">(</span><span class="n">SM80_8x8x4_F64F64F64F64_TN</span><span class="p">{},</span>
                                        <span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_1</span><span class="p">,</span> <span class="n">_1</span><span class="p">,</span> <span class="n">_1</span><span class="o">&gt;&gt;</span><span class="p">{},</span>  <span class="c1">// AtomLayout</span>
                                        <span class="n">Tile</span><span class="o">&lt;</span><span class="n">_8</span><span class="p">,</span> <span class="n">_16</span><span class="p">,</span> <span class="n">_8</span><span class="o">&gt;</span><span class="p">{});</span>         <span class="c1">// Tiler</span>

<span class="n">print</span><span class="p">(</span><span class="n">tiled_mma</span><span class="p">),</span> <span class="n">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="cm">/*print("ALayout: "), print(typename decltype(tiled_mma)::ALayout{}), print("\n");
print("BLayout: "), print(typename decltype(tiled_mma)::BLayout{}), print("\n");
print("CLayout: "), print(typename decltype(tiled_mma)::CLayout{}), print("\n");*/</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"</span><span class="se">\n</span><span class="s">MMA Atom Layout:"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">print_latex</span><span class="p">(</span><span class="n">tiled_mma</span><span class="p">);</span>
</code></pre></div></div> <blockquote> <p>如何理解？ This doesn’t actually affect the partitioning of input/output tensors because, by convention, only a single atom is ever partitioned out. It will affect the output of <code class="language-plaintext highlighter-rouge">tile_size</code> and <code class="language-plaintext highlighter-rouge">get_layoutC_MN</code> and <code class="language-plaintext highlighter-rouge">get_layoutC_TV</code> etc, which could affect any <code class="language-plaintext highlighter-rouge">TiledCopy</code> that rely on those partitioning patterns by being built on this <code class="language-plaintext highlighter-rouge">TiledMMA</code>. Regardless, you’ll find the resulting tensors from <code class="language-plaintext highlighter-rouge">partition_C</code> etc to be exactly the same since the atom partitioning is exactly the same.</p> </blockquote> <h4 id="261-映射重排">2.6.1. 映射重排</h4> <p>上面的例子中，使用的 PermutationMNK：Tile&lt;_8, _16, _8&gt;{}，<strong>T0</strong>划分的逻辑坐标不连续。使用<strong>scatter permutation</strong>，可以得到连续的逻辑坐标划分，如下代码将对 N-coord 进行重排：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TiledMMA</span> <span class="n">tiled_mma</span> <span class="o">=</span>
      <span class="n">make_tiled_mma</span><span class="p">(</span><span class="n">SM80_8x8x4_F64F64F64F64_TN</span><span class="p">{},</span>
                     <span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_1</span><span class="p">,</span> <span class="n">_1</span><span class="p">,</span> <span class="n">_1</span><span class="o">&gt;&gt;</span><span class="p">{},</span>  <span class="c1">// AtomLayout</span>
                     <span class="n">Tile</span><span class="o">&lt;</span><span class="n">_8</span><span class="p">,</span>                      <span class="c1">// Permutation on M, equivalent to 8:1, identity</span>
                          <span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_2</span><span class="p">,</span> <span class="n">_4</span><span class="p">,</span> <span class="n">_2</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">Stride</span><span class="o">&lt;</span><span class="n">_1</span><span class="p">,</span> <span class="n">_4</span><span class="p">,</span> <span class="n">_2</span><span class="o">&gt;&gt;</span><span class="p">,</span>  <span class="c1">// Permutation on N, size 16</span>
                          <span class="n">_8</span><span class="o">&gt;</span><span class="p">{});</span>  <span class="c1">// Permutation on K, equivalent to 8:1, identity</span>

<span class="n">print_latex</span><span class="p">(</span><span class="n">tiled_mma</span><span class="p">);</span>
</code></pre></div></div> <p>这将对 N 模式重排如下（影响 B、C）：</p> <ul> <li>前 2 个元素保持原位</li> <li>接下来 4 组（每组 2 个元素）被发送到 n 坐标 4</li> <li>再接下来 2 组（每组 8 个元素）被发送到 n 坐标 2</li> </ul> <p>对应的 layout 如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(2,(4,2)):(1,(4,2))
       0    1    2    3    4    5    6    7
    +----+----+----+----+----+----+----+----+
 0  |  0 |  4 |  8 | 12 |  2 |  6 | 10 | 14 |
    +----+----+----+----+----+----+----+----+
 1  |  1 |  5 |  9 | 13 |  3 |  7 | 11 | 15 |
    +----+----+----+----+----+----+----+----+
</code></pre></div></div> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">shape</span>  <span class="o">=</span> <span class="n">make_shape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">make_shape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>
<span class="k">auto</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">make_stride</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">make_stride</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>
<span class="n">print_layout</span><span class="p">(</span><span class="n">make_layout</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">)),</span> <span class="n">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
</code></pre></div></div> <p><img src="/assets/images/cuda/20250226/cute_tiled_mma/scatter_permute_SM80_8x8x4_F64F64F64F64_TN.webp" alt="Scatter Permutation 8x16x8"></p> <blockquote> <p>映射重排以获得简洁的内存布局，从而提高内存访问效率，避免 bank conflicts。</p> </blockquote> <h4 id="262-参考资料">2.6.2. 参考资料</h4> <ul> <li><a href="https://github.com/NVIDIA/cutlass/discussions/1345#discussioncomment-8485429" rel="external nofollow noopener" target="_blank">[QST] What is PermutationMNK in TiledMMA in CUTLASS 3.4 changes?</a></li> <li><a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#logical-divide-2-d-example" rel="external nofollow noopener" target="_blank">02_layout_algebra.md – Logical Divide 2-D Example</a></li> <li><a href="https://docs.nvidia.com/cutlass/latest/media/docs/cpp/cute/0t_mma_atom.html#tiledmmas" rel="external nofollow noopener" target="_blank">0t_mma_atom – TiledMMAs</a></li> </ul> <h3 id="27-universalfma">2.7. UniversalFMA</h3> <blockquote> <p>可以参考 <strong>Thakkar_BLISRetreat2023.pdf</strong> 第 26 页。</p> </blockquote> <p>UniversalFMA 是一个标量 FMA 操作的 MMAOperation 实现，定义如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">D</span><span class="p">,</span> <span class="k">class</span> <span class="nc">A</span> <span class="o">=</span> <span class="n">D</span><span class="p">,</span> <span class="k">class</span> <span class="nc">B</span> <span class="o">=</span> <span class="n">A</span><span class="p">,</span> <span class="k">class</span> <span class="nc">C</span> <span class="o">=</span> <span class="n">D</span><span class="p">&gt;</span>
<span class="k">struct</span> <span class="nc">UniversalFMA</span> <span class="p">{</span>
  <span class="k">using</span> <span class="n">DRegisters</span> <span class="o">=</span> <span class="n">D</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
  <span class="k">using</span> <span class="n">ARegisters</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
  <span class="k">using</span> <span class="n">BRegisters</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
  <span class="k">using</span> <span class="n">CRegisters</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>

  <span class="n">CUTE_HOST_DEVICE</span> <span class="k">static</span> <span class="k">constexpr</span> <span class="kt">void</span>
  <span class="n">fma</span><span class="p">(</span><span class="n">D</span><span class="o">&amp;</span> <span class="n">d</span><span class="p">,</span> <span class="n">A</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">a</span><span class="p">,</span> <span class="n">B</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">b</span><span class="p">,</span> <span class="n">C</span> <span class="k">const</span><span class="o">&amp;</span> <span class="n">c</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Forward to an ADL/cute free function for these types</span>
    <span class="k">using</span> <span class="n">cute</span><span class="o">::</span><span class="n">fma</span><span class="p">;</span>
    <span class="n">fma</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">);</span> <span class="c1">// 这里的实现就是d = a * b + c;</span>
  <span class="p">}</span>
<span class="p">};</span>
</code></pre></div></div> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">class</span> <span class="nc">D</span><span class="p">,</span> <span class="k">class</span> <span class="nc">A</span><span class="p">,</span> <span class="k">class</span> <span class="nc">B</span><span class="p">,</span> <span class="k">class</span> <span class="nc">C</span><span class="p">&gt;</span>
<span class="k">struct</span> <span class="nc">MMA_Traits</span><span class="o">&lt;</span><span class="n">UniversalFMA</span><span class="o">&lt;</span><span class="n">D</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="n">C</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
  <span class="k">using</span> <span class="n">ValTypeD</span> <span class="o">=</span> <span class="n">D</span><span class="p">;</span>
  <span class="k">using</span> <span class="n">ValTypeA</span> <span class="o">=</span> <span class="n">A</span><span class="p">;</span>
  <span class="k">using</span> <span class="n">ValTypeB</span> <span class="o">=</span> <span class="n">B</span><span class="p">;</span>
  <span class="k">using</span> <span class="n">ValTypeC</span> <span class="o">=</span> <span class="n">C</span><span class="p">;</span>

  <span class="c1">// Logical shape of the MMA</span>
  <span class="k">using</span> <span class="n">Shape_MNK</span> <span class="o">=</span> <span class="n">Shape</span><span class="o">&lt;</span><span class="n">_1</span><span class="p">,</span><span class="n">_1</span><span class="p">,</span><span class="n">_1</span><span class="o">&gt;</span><span class="p">;</span>

  <span class="c1">// Logical thread id (tid) -&gt; tidx</span>
  <span class="k">using</span> <span class="n">ThrID</span>   <span class="o">=</span> <span class="n">Layout</span><span class="o">&lt;</span><span class="n">_1</span><span class="o">&gt;</span><span class="p">;</span> <span class="c1">// 只有一个thread参与</span>

  <span class="c1">// (Logical thread id (tid), Logical value id (vid)) -&gt; coord</span>

  <span class="c1">// (tid,vid) -&gt; (m,k)</span>
  <span class="k">using</span> <span class="n">ALayout</span> <span class="o">=</span> <span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_1</span><span class="p">,</span><span class="n">_1</span><span class="o">&gt;&gt;</span><span class="p">;</span>
  <span class="c1">// (tid,vid) -&gt; (n,k)</span>
  <span class="k">using</span> <span class="n">BLayout</span> <span class="o">=</span> <span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_1</span><span class="p">,</span><span class="n">_1</span><span class="o">&gt;&gt;</span><span class="p">;</span>
  <span class="c1">// (tid,vid) -&gt; (m,n)</span>
  <span class="k">using</span> <span class="n">CLayout</span> <span class="o">=</span> <span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_1</span><span class="p">,</span><span class="n">_1</span><span class="o">&gt;&gt;</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div></div> <p>参考官方示例函数<strong>gemm_nt</strong>：<a href="https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/sgemm_sm80.cu#L478" rel="external nofollow noopener" target="_blank">https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/sgemm_sm80.cu#L478</a>，从中提取部分代码如下：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">TA</span>      <span class="o">=</span> <span class="kt">float</span><span class="p">;</span>
<span class="k">using</span> <span class="n">TB</span>      <span class="o">=</span> <span class="kt">float</span><span class="p">;</span>
<span class="k">using</span> <span class="n">TC</span>      <span class="o">=</span> <span class="kt">float</span><span class="p">;</span>
<span class="n">TiledMMA</span> <span class="n">mmaC</span> <span class="o">=</span> <span class="n">make_tiled_mma</span><span class="p">(</span><span class="n">UniversalFMA</span><span class="o">&lt;</span><span class="n">TC</span><span class="p">,</span> <span class="n">TA</span><span class="p">,</span> <span class="n">TB</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_16</span><span class="p">,</span> <span class="n">_16</span><span class="p">,</span> <span class="n">_1</span><span class="o">&gt;&gt;</span><span class="p">{});</span>  <span class="c1">// 16x16x1 TiledMMA</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"</span><span class="se">\n</span><span class="s">TiledMMA Layouts (UniversalFMA 16 16 1):"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">print</span><span class="p">(</span><span class="n">mmaC</span><span class="p">),</span> <span class="n">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="cm">/*print("ALayout: "), print(typename decltype(mmaC)::ALayout{}), print("\n");
print("BLayout: "), print(typename decltype(mmaC)::BLayout{}), print("\n");
print("CLayout: "), print(typename decltype(mmaC)::CLayout{}), print("\n");*/</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"</span><span class="se">\n</span><span class="s">MMA Atom Layout:"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">print_latex</span><span class="p">(</span><span class="n">mmaC</span><span class="p">);</span>
</code></pre></div></div> <p>Inverse TV-Layout 如下：</p> <p><img src="/assets/images/cuda/20250226/cute_tiled_mma/abc_layout_UniversalFMA_16_16_1.jpeg" alt="UniversalFMA 16x16x1 TiledMMA"></p> <p>MMA_Atom 信息如下：</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TiledMMA
  ThrLayoutVMNK:  (_1,_16,_16,_1):(_0,_1,_16,_0)
  PermutationMNK: (_,_,_)
MMA_Atom
  ThrID:      _1:_0
  Shape_MNK:  (_1,_1,_1)
  LayoutA_TV: (_1,_1):(_0,_0)
  LayoutB_TV: (_1,_1):(_0,_0)
  LayoutC_TV: (_1,_1):(_0,_0)
</code></pre></div></div> <h2 id="a-资料">A. 资料</h2> <h3 id="a1-tiledcopy-资料">A.1. TiledCopy 资料</h3> <ul> <li> <a href="https://leimao.github.io/blog/CuTe-Tiled-Copy/" rel="external nofollow noopener" target="_blank">CuTe Tiled Copy</a>：Mao Lei 博客</li> <li> <a href="https://zhuanlan.zhihu.com/p/666232173" rel="external nofollow noopener" target="_blank">cute 之 Copy抽象</a>：reed 知乎文章</li> <li> <a href="https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/tiled_copy.cu" rel="external nofollow noopener" target="_blank">cute/tutorial/tiled_copy.cu</a>：官方示例代码</li> </ul> <h3 id="a2-mma-atom-资料">A.2. MMA Atom 资料</h3> <ul> <li> <a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/0t_mma_atom.md" rel="external nofollow noopener" target="_blank">0t_mma_atom.md</a>：官方文档，MMA Atom 文档</li> <li> <a href="https://zhuanlan.zhihu.com/p/663092747" rel="external nofollow noopener" target="_blank">cute 之 MMA抽象</a>：reed 知乎文章</li> <li> <a href="https://leimao.github.io/blog/CuTe-Tiled-MMA/" rel="external nofollow noopener" target="_blank">CuTe Tiled MMA</a>：Mao Lei 博客，如何配置 TiledMMA</li> <li><a href="https://www.cs.utexas.edu/users/flame/BLISRetreat2023/slides/Thakkar_BLISRetreat2023.pdf" rel="external nofollow noopener" target="_blank">Thakkar_BLISRetreat2023.pdf</a></li> <li><a href="https://deepwiki.com/NVIDIA/cutlass/2.3-mma-atoms-and-tiledmma" rel="external nofollow noopener" target="_blank">MMA Atoms and TiledMMA</a></li> </ul> <h3 id="a3-参考代码">A.3. 参考代码</h3> <ul> <li> <a href="https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/collective/sm80_mma_multistage.hpp" rel="external nofollow noopener" target="_blank">sm80_mma_multistage.hpp</a>：官方示例代码</li> <li> <a href="https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/sgemm_sm80.cu" rel="external nofollow noopener" target="_blank">sgemm_sm80.cu</a>：官方示例代码</li> </ul> <h3 id="a3-工具">A.3. 工具</h3> <ul> <li><a href="https://www.texpage.com/" rel="external nofollow noopener" target="_blank">TeXPage</a></li> <li><a href="https://products.aspose.app/tex/viewer" rel="external nofollow noopener" target="_blank">Aspose.TeX viewer</a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/al-folio-local-deploy-ubuntu2404/">al-folio 本地部署记录（Ubuntu 24.04）</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/C++-Traits/">C++ Traits</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/%E9%81%93%E6%A0%BC%E6%8B%89%E6%96%AF-%E6%99%AE%E5%85%8B%E7%AE%97%E6%B3%95/">道格拉斯-普克算法(Douglas–Peucker algorithm)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/CMake%E6%94%AF%E6%8C%81%E5%BA%93/">CMake支持库收集</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/QGC%E9%A3%9E%E8%A1%8C%E5%89%8D%E6%A3%80%E6%9F%A5/">QGC代码架构解析：飞行前检查（起飞条件）</a> </li> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Roderick Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>